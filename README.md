# Semiosis: Unit Testing for Documentation Quality

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Python](https://img.shields.io/badge/python-3.9%2B-blue.svg)
![Status](https://img.shields.io/badge/status-alpha-red.svg)

Semiosis is an open-source framework for measuring the semantic quality of static documentation and context systems. Think "unit testing for your knowledge base" - Semiosis reveals how much information is redundant, what's critical, and where your documentation breaks down.

## ğŸ¯ Why Semiosis?

**The Problem**: You've built extensive documentation (DBT projects, API docs, knowledge bases) but don't know if it's actually good. Is there redundancy? What happens if parts go missing? Is it token-efficient?

**The Solution**: Semiosis measures **context system quality** using standardized LLM probes to evaluate:
- **Completeness**: Does your documentation cover all necessary concepts?
- **Redundancy**: How much can you remove while maintaining performance?  
- **Semantic Density**: How much information per documentation unit?
- **Robustness**: How gracefully does performance degrade as context is removed?
- **Critical Boundaries**: What's the minimum viable documentation set?

## ğŸš€ Vision

When complete, Semiosis will provide comprehensive documentation quality analysis:

```bash
# Analyze your DBT project documentation quality
semiosis evaluate \
    --context dbt \
    --context-args project_path=./my_dbt_project \
    --environment text-to-sql \
    --interventions progressive_removal,schema_corruption

# Expected results: context quality report
# ğŸ“Š Baseline Performance: 94% (excellent documentation)
# ğŸ¯ Semantic threshold: Î·_c = 0.35 (robust to 65% removal)
# ğŸ’ Critical components: schema.yml files (high impact)
# ğŸ“ˆ Redundancy: Column descriptions (medium overlap)
# ğŸ† Benchmark: 75th percentile vs industry average
```

## ğŸ—ï¸ Architecture

Semiosis provides a modular framework for context quality measurement:

- **ğŸŒ Environments**: Define evaluation scenarios (text-to-SQL, code generation, custom domains)
- **ğŸ¤– Standardized Probes**: Built-in LLM agents as measurement instruments
- **ğŸ“š Context Systems**: Integration with documentation sources (DBT, API docs, knowledge bases)
- **âš¡ Interventions**: Systematic context modifications (removal, corruption, reordering)
- **ğŸ“ˆ Quality Engine**: Mathematical framework for measuring semantic information density

## ğŸ”¬ Planned Use Cases

### Documentation Optimization
```bash
# Find minimal documentation set for reliable performance
semiosis evaluate --context dbt --interventions progressive_removal
# Expected: Need only 40% of semantic models for 90% accuracy
```

### Pre-Deployment Validation
```bash
# Test documentation robustness before agent deployment
semiosis evaluate --interventions corruption,missing_schemas,outdated_docs
# Expected: Performance drops to 60% with 30% schema corruption
```

## ğŸ“Š Expected Results

- **ğŸ“ˆ Quality Curves**: How performance degrades with documentation removal
- **ğŸ¯ Semantic Thresholds**: Critical information boundaries (Î·_c values)
- **ğŸ’ Component Analysis**: Which documentation sections are most valuable
- **ğŸ“Š Redundancy Maps**: What information overlaps and can be consolidated
- **ğŸ† Benchmarking**: How your context compares to industry standards
- **âš¡ Intervention Impact**: Quantified effects of specific documentation changes

## ğŸ› ï¸ Planned Integrations

### Standardized Measurement Probes
- **Anthropic Claude**: Claude-3.5 Sonnet, Claude-3 Haiku with logprobs extraction
- **OpenAI**: GPT-4, GPT-4o with token probability measurement
- **Open Source**: Llama, Mistral via Hugging Face Inference API
- **Cloud Platforms**: AWS Bedrock, Google Vertex AI for enterprise deployment

### Evaluation Environments  
- **Text-to-SQL**: Spider 2.0, BIRD-SQL datasets for database query generation
- **Code Generation**: Programming task evaluation with execution validation
- **Custom Domains**: YAML-configurable environments for any documentation type

### Documentation Sources
- **DBT Projects**: Schema definitions, model docs, semantic layer analysis
- **API Documentation**: OpenAPI specs, endpoint descriptions, parameter definitions
- **Knowledge Bases**: Markdown files, wikis, technical documentation
- **Custom Sources**: Any structured documentation via plugins

## ğŸ§® Mathematical Foundation

Semiosis will implement a rigorous mathematical framework based on semantic information theory:

```
Agent state:              ğš = (q, y, â„“, c, b, Î¸)
Environment state:        ğ = (D, Q, T)  
Context system:           ğ’®_Î· = [sâ‚, â€¦, sâ‚™]
Intervention:             ğ’®_Î·' = ğ’®_Î· + s_{n+1}
Agent output:             p_Î¸(y | q, D, ğ’®_Î·)
Token probability:        p_Î¸(táµ¢ | t_{<i}, q, D, ğ’®_Î·)
Log-likelihood:           LL_Î·(t) = Î£áµ¢ log p_Î¸(táµ¢ | t_{<i}, q, D, ğ’®_Î·)
Cross-entropy:            H_Î· = ğ”¼[âˆ’LL_Î·(t(q))]
Trust update:             â„“' = â„“ + f(LL(t))
Budget update:            b' = b âˆ’ c + g(â„“')
Viability:                V(Î·) = Pr(â„“ > â„“_min âˆ§ b > 0)
Semantic threshold:       Î·_c = inf{Î· | V(Î·) â‰¤ Â½V(1)}
```

Where agents maintain **trust** (â„“) through performance and **budget** (b) through resource management, with **viability** measuring sustainable operation probability.

## ğŸ¤ Contributing

We welcome contributions! Key areas for community involvement:

- **ğŸŒ Environments**: Create evaluation scenarios for specific domains  
- **ğŸ“š Context Systems**: Integrate new semantic layer/knowledgebase/documentation technologiess

See our [Contributing Guide](CONTRIBUTING.md) for detailed instructions.

### Development Setup

```bash
git clone https://github.com/AnswerLayer/semiosis.git
cd semiosis
pip install -e ".[dev]"
# Note: Core framework still in development - tests coming soon
```

## ğŸ”‘ API Key Setup

Semiosis requires API keys for the standardized LLM measurement probes. Set up your credentials:

### Anthropic Claude (Recommended)

```bash
# Option 1: Environment variable (recommended)
export ANTHROPIC_API_KEY=sk-ant-api03-your-key-here

# Option 2: Use .env file (copy from template)
cp .env.example .env
# Edit .env with your actual API keys

# Option 3: Configuration file
# config.yaml
agent:
  type: anthropic
  args:
    api_key: sk-ant-api03-your-key-here
    model: claude-3-5-sonnet-20241022  # or claude-opus-4-5
```

### Usage Examples

```bash
# Analyze your DBT project with Claude
export ANTHROPIC_API_KEY=sk-ant-api03-...
semiosis evaluate \
    --context dbt \
    --context-args project_path=./my_dbt_project \
    --environment text-to-sql

# Use latest Claude 4.5 models  
semiosis evaluate \
    --context dbt \
    --context-args project_path=./my_dbt_project \
    --agent-args model=claude-opus-4-5,temperature=0.1
```

### Supported Models

- **Claude 4.x**: `claude-opus-4-5`, `claude-sonnet-4-5`, `claude-haiku-4-5` (latest)
- **Claude 3.5**: `claude-3-5-sonnet-20241022`, `claude-3-5-haiku-20241022` (stable)
- **Claude 3**: `claude-3-opus-20240229`, `claude-3-sonnet-20240229` (legacy)

### Getting API Keys

1. **Anthropic**: Visit [console.anthropic.com](https://console.anthropic.com) to get your API key
2. **Pricing**: See [anthropic.com/pricing](https://www.anthropic.com/pricing) for current rates

âš ï¸ **Security**: Never commit API keys to version control. Use environment variables or local config files.

## ğŸ“š Citation

If you use Semiosis in your research, please cite:

```bibtex
@software{semiosis2025,
  title={Semiosis: Evaluate Semantic Layers for AI Agent Performance},
  author={AnswerLayer Team},
  year={2025},
  url={https://github.com/AnswerLayer/semiosis}
}
```

## ğŸ“– References

This framework builds on foundational work in semantic information theory:

**[1]** Kolchinsky, A. and Wolpert, D.H. Semantic information, autonomous agency, and nonequilibrium statistical physics. *New Journal of Physics*, 20(9):093024, 2018. [arXiv:1806.08053](https://arxiv.org/pdf/1806.08053)

**[2]** Sowinski, D.R., Balasubramanian, V., and Kolchinsky, A. Semantic information in a model of resource gathering agents. *Physical Review E*, 107(4):044404, 2023. [arXiv:2304.03286](https://arxiv.org/pdf/2304.03286)

**[3]** Balasubramanian, V. and Kolchinsky, A. Exo-Daisy World: Revisiting Gaia Theory through an Informational Architecture Perspective. *Planetary Science Journal*, 4(12):236, 2023. [PSJ](https://iopscience.iop.org/article/10.3847/PSJ/ade310)

**[4]** Sowinski, D.R., Frank, A., and Ghoshal, G. Information-theoretic description of a feedback-control Kuramoto model. *Physical Review Research* 6, 043188, 2024. [arXiv:2505.20315](https://arxiv.org/pdf/2505.20315)

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file for details.

## ğŸ”— Links

- **[GitHub Issues](https://github.com/AnswerLayer/semiosis/issues)** - Bug reports and feature requests
- **[GitHub Discussions](https://github.com/AnswerLayer/semiosis/discussions)** - Community discussion  
- **[AnswerLayer](https://answerlayer.com)** - Parent organization

---

**Status**: Alpha - Active development. APIs may change.

**Roadmap**: See [GitHub Issues](https://github.com/AnswerLayer/semiosis/issues/1) for current development plan.